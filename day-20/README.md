# AI Voice Agent - Day 20

## ðŸ¤– About the Project

This project is a sophisticated AI Voice Agent that seamlessly integrates real-time speech-to-text, a powerful language model, and text-to-speech capabilities. In this iteration, we've enhanced the agent's responsiveness by streaming the LLM's output directly to a Text-to-Speech service (Murf AI) via WebSockets. This allows for a more natural and interactive conversational experience, as the user can hear the AI's response as it's being generated.

## âœ¨ Key Features

  * **Real-time Transcription**: Utilizes AssemblyAI for accurate and fast speech-to-text conversion.
  * **Intelligent Conversation**: Leverages Google's Gemini 1.5 Flash model for generating human-like responses.
  * **Streaming Audio Synthesis**: Implements WebSocket communication with Murf AI to stream the LLM's text response and receive synthesized audio in real-time.
  * **Asynchronous Processing**: Employs `asyncio` and `threading` to handle concurrent operations, ensuring a non-blocking and smooth user experience.
  * **Static Context ID**: Uses a static `context_id` for Murf AI WebSocket requests to prevent context limit errors during prolonged conversations.

-----

## ðŸŽ¥ Demo

A demonstration of this project would show a user speaking into their microphone. The user's speech is transcribed in real-time on the screen. Almost immediately after the user finishes speaking, the AI's response is streamed to the console as base64 encoded audio chunks, and simultaneously, the text of the response appears on the screen as it is generated by the LLM.

-----

## ðŸ”„ Changes Made

The primary focus of this update was to integrate Murf AI's streaming TTS service with the Gemini LLM's streaming response. Here's a breakdown of the key modifications:

### `services/llm.py`

  * **`get_llm_streaming_response_with_murf` function**: This new asynchronous function orchestrates the entire streaming process.

      * It establishes a WebSocket connection to the Murf AI API.
      * It sends the streaming text output from the Gemini LLM sentence by sentence to Murf.
      * To address potential context limit issues, a static `context_id` is now sent with each request to Murf, ensuring that a single context is used throughout the session.
      * It concurrently receives the synthesized audio from Murf in the form of base64 encoded chunks.

  * **`receive_loop` function**: This helper function is responsible for listening to the WebSocket for incoming audio chunks from Murf and collecting them.

### `main.py`

  * **WebSocket Endpoint (`/ws`)**: The main WebSocket endpoint has been updated to incorporate the new LLM and TTS streaming logic.
  * **Asynchronous and Threaded Execution**:
      * A new synchronous wrapper function, `process_llm_with_murf_sync`, was created to call the asynchronous `process_llm_with_murf_async` function from within the existing synchronous parts of the application.
      * This is achieved by running the async function in a separate thread with its own event loop, preventing any blocking of the main application.
  * The endpoint now prints the base64 encoded audio chunks received from Murf directly to the console.

-----

## ðŸš€ Getting Started

### Prerequisites

  * Python 3.9+
  * API keys for:
      * AssemblyAI
      * Google Gemini
      * Murf AI

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/ai-voice-agent.git
    cd ai-voice-agent
    ```
2.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
3.  **Set up your environment variables:**
    Create a `.env` file in the project root and add your API keys:
    ```
    ASSEMBLYAI_API_KEY="your_assemblyai_api_key"
    GEMINI_API_KEY="your_gemini_api_key"
    MURF_API_KEY="your_murf_api_key"
    ```

### Running the Application

1.  **Start the server:**
    ```bash
    uvicorn main:app --reload
    ```
2.  **Open your browser** and navigate to `http://127.0.0.1:8000`.
