# ğŸ™ï¸ Murf AI Voice Agent Challenge

Welcome to my journey through the **Murf AI Voice Agent 30-Day Challenge**!
I'm building a smart and interactive **voice agent** using Murf AI's powerful TTS capabilities and integrating it with real-time tech like **AssemblyAI, FastAPI**, and **LLM APIs**.

---

### ğŸ—“ï¸ Day 1 â€“ Kickoff & Setup

- ğŸš€ Joined the **Murf AI Voice Agent Challenge**
- ğŸ§  Explored the challenge format, goals, and tools
- ğŸ’» Set up the base project using **FastAPI**
- ğŸ” Registered and tested the **Murf API key**
- ğŸ‰ Successfully generated my **first TTS audio** with Murf AI

---

### ğŸ—“ï¸ Day 2 â€“ TTS API Integration

- ğŸ” Connected **Murf's TTS API** to FastAPI backend
- ğŸ§ª Built a basic UI with text input and audio playback
- ğŸ§ Achieved full **text-to-speech cycle** in browser
- ğŸ› ï¸ Handled errors gracefully on both front and back end
- ğŸ“¢ Shared my progress on LinkedIn with `#30DayVoiceAgent`

---

### ğŸ—“ï¸ Day 3 â€“ Voice Agent UX

- ğŸ–Œï¸ Polished the UI with improved design (HTML/CSS)
- ğŸ”„ Refactored API flow for smoother UX
- ğŸ’¡ Learned how to make the voice interaction feel more natural
- ğŸ™Œ Thanked **Murf AI** publicly for enabling student creativity

---

### ğŸ—“ï¸ Day 4 â€“ Echo Bot ğŸ¤

- ğŸª Added a brand-new feature: **Echo Bot** section in the UI
- ğŸ§© Used the browserâ€™s **MediaRecorder API** to:

  - Start and stop mic recordings
  - Instantly play back recorded audio

- ğŸ§  Learned how to work with real-time audio in the browser
- âœ¨ This will serve as the foundation for future **speech input** integration

---

### ğŸ—“ï¸ Day 5 â€“ Audio Upload + Server Integration â˜ï¸

- âºï¸ Extended the Echo Bot to **upload audio to my Python server**
- ğŸ› ï¸ Built a new `/upload` API in **FastAPI** to:

  - Accept audio blob from frontend
  - Save it in an `/uploads` folder
  - Return file **name**, **type**, and **size**

- ğŸ”” Added a real-time **status message on UI** after upload
- ğŸ”ƒ Improved end-to-end interactivity from mic â†’ server â†’ playback

---

### ğŸ—“ï¸ Day 6 â€“ Transcription Integration âœï¸

- ğŸ§µ Created `/transcribe/file` endpoint on backend
- ğŸ“¤ Accepts audio, returns **transcription** via **AssemblyAI**
- ğŸ–¥ï¸ Integrated transcription into frontend UI
- ğŸ“œ Now have **record voice â†’ upload â†’ transcribe â†’ display text** flow

---

### ğŸ—“ï¸ Day 7 â€“ Voice-to-Voice with `/tts/echo` ğŸ¤ğŸ”„ğŸ™ï¸

- ğŸ†• Backend endpoint: `/tts/echo`
- ğŸ™ï¸ Flow:

  1. Accept audio â†’ transcribe (AssemblyAI)
  2. Send text to Murf AI â†’ generate new voice
  3. Return voice file URL to client

- ğŸ”„ Full voice-to-voice pipeline now works: **User speaks â†’ Server transcribes â†’ Murf re-voices â†’ Client plays**

---

### ğŸ—“ï¸ Day 8 â€“ Building LLM Query Endpoint ğŸ§ ğŸ’¬

- ğŸ†• Added a brand-new backend endpoint: **`/llm/query`** in FastAPI
- ğŸ“© This endpoint:

  - Accepts a **JSON payload** containing `text` from the frontend
  - Sends the text to **Google Gemini API**
  - Returns the AI-generated response in **JSON format**

- âš¡ Used the `gemini-1.5-flash` model for quick replies
- ğŸ› ï¸ Created a helper function `getResponseFromGemini()` to keep code clean
- ğŸš« Implemented error handling for API issues and model mismatches
- ğŸ’¡ This is the first step towards a **conversational AI** that can handle natural queries

---

### ğŸ—“ï¸ Day 9 â€“ Audio-to-Audio AI Conversation ğŸ¤ğŸ¤–ğŸ™ï¸

- ğŸ”„ Upgraded the `/llm/query` endpoint to **accept audio recordings** directly from the browser
- ğŸ“‹ New flow:

  1. User records voice in browser
  2. Audio is sent to backend as `multipart/form-data`
  3. **AssemblyAI** transcribes the speech into text
  4. Transcription is sent to **Google Gemini API** for a reply
  5. AI reply is sent to **Murf AI** for lifelike TTS output
  6. Backend returns the generated audio to the frontend

- ğŸ§ On the frontend, the new voice plays instantly after generation
- âœ¨ Now the AI can **listen and talk back** without needing any text input

---

### ğŸ—“ï¸ Day 10 â€“ Session-Based Chat Memory ğŸ—‚ï¸ğŸ—£ï¸

- ğŸ§  Added **context awareness** so the AI remembers what was said earlier in the conversation
- ğŸ†• New backend endpoint: **`/agent/chat/{session_id}`**
- ğŸ“‹ Flow:

  1. Accepts **audio file** from client
  2. Transcribes it with **AssemblyAI**
  3. Saves message into **session chat history** keyed by `session_id`
  4. Sends **full conversation history** to Gemini for context-rich replies
  5. Adds AI reply to session memory
  6. Converts reply to speech with Murf AI and sends it back

- ğŸ¯ Result: **Smooth, natural, and context-aware conversations** with the AI agent

---

### ğŸ—“ï¸ Day 11 â€“ Robust Error Handling ğŸ›¡ï¸âš™ï¸

- ğŸ”’ Added **try/except** blocks in FastAPI to catch backend errors
- ğŸ› ï¸ Added **try/catch** in JavaScript to show clear error messages to users
- ğŸ“¢ User now gets **friendly alerts** instead of cryptic error codes
- ğŸ“‰ Reduced app crashes during network/API failures
- âœ… A more **reliable and user-friendly** experience overall

---

### ğŸ—“ï¸ Day 12 â€“ Conversational Agent UI Revamp ğŸ¨ğŸ–¥ï¸

- ğŸ¯ Focused on **polishing the user interface** for better UX
- âœ¨ Key improvements:

  - **One-tap recording**: Start/stop with a single button
  - **Animated mic button**: Visual feedback while recording
  - **Auto-play replies**: No extra clicks to hear AIâ€™s voice
  - **Mobile responsive design** for on-the-go testing
  - Removed unused sections to keep UI clean and minimal

- ğŸ“¸ Updated UI screenshot:
  ![alt text](image.png)
- ğŸš€ Now the voice agent **feels like a real app**, not just a prototype

---

### ğŸ—“ï¸ Day 13 â€“ Readme ğŸ¨ğŸ–¥ï¸

-- Updated Readme.md

---

### ğŸ—“ï¸ Day 14 â€“ Folder Structure ğŸ¨ğŸ–¥ï¸

- Structured Folder setup
- created helper functions

---

### ğŸ—“ï¸ Day 15 â€“ Websockets ğŸ¨ğŸ–¥ï¸

- Today i created a endpoint /ws
- checked it on postman
- and it's working

---

### ğŸ—“ï¸ Day 15 â€“ Websockets ğŸ¨ğŸ–¥ï¸

Hereâ€™s a polished **LinkedIn post** draft for **Day 16 of your Voice Agent challenge** ğŸ‘‡

---

# ğŸ™ï¸ Day 16

ğŸ”¹ **What I built**
I implemented a system where the **client records audio and streams it live to the server** using **WebSockets**. Instead of collecting all chunks locally, the browser sends small packets of audio data to the server at regular intervals.

On the **server side**, these binary audio chunks are received and saved directly into a file â€” no transcription, LLM, or TTS yet, just **pure audio capture and streaming**.

ğŸ”¹ **Why it matters**

- Enables **real-time audio handling**
- Scales better for longer recordings (no huge memory usage on client)
- Lays the foundation for real-time features like **live transcription, voice commands, or streaming TTS** in future iterations

---

# ğŸ™ï¸ Day 17 â€“ Streaming Transcription with AssemblyAI

## ğŸ”¹ Overview

Yesterday (Day 16), I implemented **real-time audio streaming** from the client to the server using **WebSockets**.  
Today, I extended that system by integrating **AssemblyAIâ€™s Python SDK** to **transcribe the incoming audio stream**.

Instead of only saving raw audio, the server now connects to AssemblyAIâ€™s **Streaming API**, transcribes the audio in real time, and prints the transcription to the console (or directly on the UI).

---

## ğŸ”¹ What I Built

1. **Client Side**

   - Captures microphone input
   - Streams audio chunks to the server via WebSockets at regular intervals

2. **Server Side**
   - Receives binary audio chunks from the client
   - Uses **AssemblyAIâ€™s Python SDK** to stream audio to their API
   - Prints real-time transcription to the console (can also push to the UI)

---

## ğŸ”¹ Why It Matters

- Converts speech â†’ text in **real time**
- Makes the system **interactive and dynamic**
- Lays the foundation for **voice commands, dialogue systems, and searchable transcripts**

---

## ğŸ”¹ Next Steps (Day 18 Preview)

AssemblyAIâ€™s Streaming API also supports **turn detection**:

- Detects when a user **stops speaking**
- Sends a **WebSocket event to the client** signaling the end of a turn
- Displays the **final transcription** on the UI at the end of each turn

This will make the voice agent feel more **natural and conversational**.

---

## âš™ï¸ What Youâ€™ll Need

- **FastAPI** (Python)
- **Murf AI API key**
- **AssemblyAI API key**
- **Google Gemini API key**
- HTML, CSS, JS frontend
- `.env` file to store keys

---

## ğŸ’¡ Tools I'm Using

| Tool             | Purpose                             |
| ---------------- | ----------------------------------- |
| Murf AI          | Text-to-Speech (TTS)                |
| FastAPI          | Backend API server                  |
| HTML/CSS/JS      | UI for interaction and playback     |
| MediaRecorder    | Echo Bot mic capture + playback     |
| FormData         | Uploading audio blob to the backend |
| AssemblyAI / STT | Transcribing recorded audio         |
| Gemini API       | AI-generated conversation           |

---

# ğŸ›  Installation & Run Instructions

### ğŸ“‚ Project Structure

```
â”œâ”€â”€ ğŸ“ .git/ ğŸš« (auto-hidden)
â”œâ”€â”€ ğŸ“ .venv/ ğŸš« (auto-hidden)
â”œâ”€â”€ ğŸ“ Agent/
â”‚   â”œâ”€â”€ ğŸ“ Routes/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”‚   â””â”€â”€ ğŸ agent_chat.py
â”‚   â”œâ”€â”€ ğŸ“ Services/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”‚   â”œâ”€â”€ ğŸ Gemini_service.py
â”‚   â”‚   â”œâ”€â”€ ğŸ Stt_service.py
â”‚   â”‚   â””â”€â”€ ğŸ Tts_service.py
â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”œâ”€â”€ ğŸ“ utils/
â”‚   â”‚   â”œâ”€â”€ ğŸ“ __pycache__/ ğŸš« (auto-hidden)
â”‚   â”‚   â””â”€â”€ ğŸ logging.py
â”‚   â”œâ”€â”€ ğŸŒ index.html
â”‚   â”œâ”€â”€ ğŸ main.py
â”‚   â”œâ”€â”€ ğŸ“„ script.js
â”‚   â”œâ”€â”€ ğŸ¨ style.css
â”‚   â””â”€â”€ ğŸ“„ tempCodeRunnerFile.js
â”œâ”€â”€ ğŸ”’ .env ğŸš« (auto-hidden)
â”œâ”€â”€ ğŸš« .gitignore
â”œâ”€â”€ ğŸ“– README.md
â”œâ”€â”€ ğŸ“„ Requirement.txt
â””â”€â”€ ğŸ–¼ï¸ image.png

```

---

### ğŸ”‘ API Keys

Create `.env` file in root:

```env
MURF_API_KEY=your_murf_api_key
ASSEMBLY_API_KEY=your_assemblyai_api_key
GEMINI_API_KEY=your_gemini_api_key
```

---

### ğŸ“¥ Installation Steps

1ï¸âƒ£ **Clone the repo**

```bash
git clone https://github.com/Vishalpandey1799/Murf-AI-Voice-Agent.git
cd Murf-AI-Voice-Agent
```

2ï¸âƒ£ **Create and activate virtual environment**

- **Windows:**

```bash
python -m venv .venv
.venv\Scripts\activate
```

- **Mac/Linux:**

```bash
python3 -m venv .venv
source .venv/bin/activate
```

3ï¸âƒ£ **Install dependencies**

```bash
pip install -r requirement.txt
```

4ï¸âƒ£ **Run the FastAPI server**

```bash
uvicorn main:app --reload
```

---

## ğŸ™Œ Special Thanks

Huge thanks to **Murf AI** for organizing this amazing challenge and encouraging builders to explore the world of voice-first interfaces.
Your tools are enabling the next generation of interactive agents ğŸ’œ

---

## ğŸ”— Follow My Progress

ğŸ“ Catch my updates on LinkedIn with: [#30DayVoiceAgent](https://www.linkedin.com/in/vishal-kumar-3835a9330/)
Letâ€™s build cool voice stuff together!

---
